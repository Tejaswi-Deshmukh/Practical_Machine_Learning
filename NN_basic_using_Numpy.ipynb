{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "esxX1tUruafM"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8xTkG-nuafX"
      },
      "source": [
        "# Activation Functions\n",
        "\n",
        "Theshold  ( bias)\n",
        "\n",
        "            f(x) = 0 for x< t and 1 for x>= t\n",
        "\n",
        "Linear\n",
        "\n",
        "            f(x) = x\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "            f(x) = 1/(1 + e^-x )           \n",
        "\n",
        "TanH\n",
        "\n",
        "            f(x) = tanh(x)\n",
        "            \n",
        "ReLU (Rectified Linear Unit)\n",
        "\n",
        "            f(x) = max(0,x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6juwGAJWuafY",
        "outputId": "5ddc5dfa-ff29-4e73-b90a-2202db920435"
      },
      "source": [
        "I = np.array([1,2,4])\n",
        "W = np.array([0.5,0.3,0.1])\n",
        "I*W"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5, 0.6, 0.4])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18MRRmOYuafa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f920b865-b968-4571-fda4-fe9f0a1315ae"
      },
      "source": [
        "X = sum(I*W)# (I*W).sum()\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZpciK2euafb"
      },
      "source": [
        "def sigmoid (x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(y_pred): # sigmoid derivative is reduced to value y_pred * (1-y_pred)\n",
        "    return y_pred * (1 - y_pred) # here y_pred = 1/(1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwvzmrBsuafb"
      },
      "source": [
        "def relu(x):\n",
        "    return max(0,x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    \"\"\"\n",
        "    df/dx = 0 for x < 0 and 1 for x>0\n",
        "    for x==0 derivative is undefined\n",
        "    FOR ALL PRACTICAL PURPOSES\n",
        "    in implementation when x==0\n",
        "    we can return 0 / 1\n",
        "    Here we decide to return 0\n",
        "    \"\"\"\n",
        "    result = None\n",
        "    if x > 0:\n",
        "        result = 1\n",
        "    elif x <= 0:\n",
        "        result = 0\n",
        "#     else:\n",
        "#         print(\"undefined\")\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-gUZvwbuafc"
      },
      "source": [
        "#Input datasets\n",
        "## XOR Gate Data\n",
        "#X or known variables\n",
        "inputs = np.array([[0,0],[0,1],[1,0],[1,1],[0,0],[0,1],[1,0],\\\n",
        "                   [1,1],[0,0],[0,1],[1,0],[1,1]])\n",
        "inputs = inputs.T\n",
        "#Y or un-known variables\n",
        "expected_output = np.array([[0],[1],[1],[0],[0],[1],[1],[0],\\\n",
        "                           [0],[1],[1],[0]])\n",
        "expected_output = expected_output.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiXVPlLGuafc"
      },
      "source": [
        "# Configure the Neural Network\n",
        "\n",
        "## Input Layer -->\n",
        "Here neurons are just place holders , there is no processing\n",
        "\n",
        "ONLY input is taken and given to first hidden layer\n",
        "\n",
        "## No of Input Neurons in input layer::\n",
        "Same as no of input columns\n",
        "\n",
        "## No of Output Neurons in output layer\n",
        "\n",
        "+ Based on Type of the output whether classification or regression problem\n",
        "\n",
        "1. Classification ::\n",
        "\n",
        "    a. With One hot encoded  ::\n",
        "            No of output neurons = No of output classes\n",
        "    b. Without One hot encoding\n",
        "            No of output neurons = 1\n",
        "\n",
        "2. Regression ::\n",
        "\n",
        "    a. If one output variable :\n",
        "            No of output Neurons = 1\n",
        "    b. If multiple output variables :\n",
        "            No of output Neurons = No of output variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aSaprJVuafd"
      },
      "source": [
        "# How to decide no of Neurons in input layer and output layer\n",
        "# Thumb rules for hidden layers\n",
        "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,4,1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfriz80wuafd"
      },
      "source": [
        "# How to decide Hidden Layer Nuerons\n",
        "\n",
        "#### NOTE ::: There is NO thumb rules for this !!!!\n",
        "\n",
        "## You are required to tune :::\n",
        "\n",
        "### No of Hidden Layers\n",
        "### No of Neurons in each hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFeXguNfuafe",
        "outputId": "9b3dd1d1-f026-4fc8-afea-031d772f275a"
      },
      "source": [
        "inputs.shape, expected_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 12), (1, 12))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzzI5q4iuafe"
      },
      "source": [
        "# How To initialize Weights\n",
        "\n",
        "+ Method 1: Sample weights randomly from uniform distribution ( and values between 0 to 1) In uniform distribution all possible values have same probability\n",
        "+ Method 2 : Use pre existing values (Transfer Learning)\n",
        "+ Method 3 : Constant Values --> Very old method, not used now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soWtFR5Vuaff"
      },
      "source": [
        "#Random weights and bias initialization\n",
        "hidden_weights = np.random.uniform( size=(hiddenLayerNeurons,inputLayerNeurons))\n",
        "hidden_bias =np.random.uniform(size=(hiddenLayerNeurons,1))\n",
        "output_weights = np.random.uniform(size=(outputLayerNeurons,hiddenLayerNeurons))\n",
        "output_bias = np.random.uniform(size=(outputLayerNeurons,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaro8j56uaff",
        "outputId": "691fb4ac-500b-4785-b297-2defab62489f"
      },
      "source": [
        "hidden_weights, hidden_weights.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.07630829, 0.77991879],\n",
              "        [0.43840923, 0.72346518],\n",
              "        [0.97798951, 0.53849587],\n",
              "        [0.50112046, 0.07205113]]),\n",
              " (4, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4Q-e2h_uafg",
        "outputId": "ad729908-08fc-45b3-c515-069ff76027dd"
      },
      "source": [
        "hidden_bias"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.26843898],\n",
              "       [0.4998825 ],\n",
              "       [0.67923   ],\n",
              "       [0.80373904]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0seBwGU4uafh",
        "outputId": "5e0feb69-5a49-4ef1-bb7b-335947aa301e"
      },
      "source": [
        "output_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38094113, 0.06593635, 0.2881456 , 0.90959353]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-q9oc-puafh",
        "outputId": "20486337-fde1-4c40-d7bb-8716d1d1e27d"
      },
      "source": [
        "output_bias"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.21338535]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO7H9yl5uafi"
      },
      "source": [
        "# How to Decide no of Epochs\n",
        "\n",
        "## Early Stopping Method\n",
        "\n",
        "In coding enable stopage of training when validation loss increases ( over fitting starts)\n",
        "\n",
        "## Manual Method\n",
        "1. Have a fixed no to begin with ( generally i prefer 100 epochs)\n",
        "\n",
        "2. Check for underfitting or overfitting\n",
        "\n",
        "3. When underfitting --> Increase the no of epochs\n",
        "\n",
        "4. When overfitting --> Decrease the no of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AASnkKAiuafi"
      },
      "source": [
        "epochs = 150000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCzez1aUuafj"
      },
      "source": [
        "lr = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfOJzTZDuafj",
        "outputId": "4f25f5de-f3de-4fb7-97dc-b129385ceb4d"
      },
      "source": [
        "print(\"Initial hidden weights: \",end='')\n",
        "print(*hidden_weights)\n",
        "print(\"Initial hidden biases: \",end='')\n",
        "print(*hidden_bias)\n",
        "print(\"Initial output weights: \",end='')\n",
        "print(*output_weights)\n",
        "print(\"Initial output biases: \",end='')\n",
        "print(*output_bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial hidden weights: [0.07630829 0.77991879] [0.43840923 0.72346518] [0.97798951 0.53849587] [0.50112046 0.07205113]\n",
            "Initial hidden biases: [0.26843898] [0.4998825] [0.67923] [0.80373904]\n",
            "Initial output weights: [0.38094113 0.06593635 0.2881456  0.90959353]\n",
            "Initial output biases: [0.21338535]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RzFUijZyF9_"
      },
      "source": [
        "# Training\n",
        "\n",
        "## Error Function:\n",
        "\n",
        "Ideal Error should be a perfect convex curve with single global minima (no local minimas)\n",
        "\n",
        "Error curve is assumed to be continuous and a differentiable function\n",
        "\n",
        "#Weight Update Rule\n",
        "\n",
        "W = W - lr * delta_change\n",
        "\n",
        "where\n",
        "\n",
        "delta_change = error_at_layer * slope_of_layer_activation\n",
        "\n",
        "lr is learning rate\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bjDbZu0uafj"
      },
      "source": [
        "#Training algorithm\n",
        "for _ in range(epochs):\n",
        "    #Forward Propagation\n",
        "    hidden_layer_summation = np.dot(hidden_weights,inputs)\n",
        "    hidden_layer_summation += hidden_bias\n",
        "    hidden_layer_output = sigmoid(hidden_layer_summation)\n",
        "\n",
        "\n",
        "    output_layer_summation = np.dot(output_weights,hidden_layer_output)\n",
        "    output_layer_summation += output_bias\n",
        "    predicted_output = sigmoid(output_layer_summation)\n",
        "\n",
        "    #Backpropagation\n",
        "    # derivative of loss wrt yp\n",
        "    error = predicted_output - expected_output\n",
        "    #derivative at output layer -(ya - yp) * (yp*(1-yp))\n",
        "    d_output = error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "    error_hidden_layer = output_weights.T.dot(d_output)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    #Updating Weights and Biases\n",
        "    # derivate of loss wrt weight = inputs * derivative at layer\n",
        "    o_derivative= hidden_layer_output.dot(d_output.T)\n",
        "    output_weights -= o_derivative.T * lr\n",
        "    output_bias -= np.sum(d_output,axis=1,keepdims=True) * lr\n",
        "    # derivate of loss wrt weight = inputs * derivative at layer\n",
        "    h_derivative = inputs.dot(d_hidden_layer.T)\n",
        "    hidden_weights -= h_derivative.T * lr\n",
        "    hidden_bias -= np.sum(d_hidden_layer,axis=1,keepdims=True) * lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpPwaml4uafk",
        "outputId": "42b8e3d5-99db-48d1-a60a-e92427db9088"
      },
      "source": [
        "print(\"\\nOutput from neural network after\",epochs,\" epochs: \")\n",
        "for e,p in zip(expected_output, predicted_output):\n",
        "    print(\"expected \", e, \"predicted\",p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output from neural network after 150000  epochs: \n",
            "expected  [0 1 1 0 0 1 1 0 0 1 1 0] predicted [0.00235981 0.99809911 0.99802359 0.00177694 0.00235981 0.99809911\n",
            " 0.99802359 0.00177694 0.00235981 0.99809911 0.99802359 0.00177694]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFlBchT2uafl",
        "outputId": "5133689d-4fee-4fa2-ec59-1b3289ed2a35"
      },
      "source": [
        "print(\"Final hidden weights: \",end='')\n",
        "print(hidden_weights)\n",
        "print(\"Final hidden bias: \",end='')\n",
        "print(hidden_bias)\n",
        "print(\"Final output weights: \",end='')\n",
        "print(output_weights)\n",
        "print(\"Final output bias: \",end='')\n",
        "print(output_bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final hidden weights: [[0.06160714 0.76685929]\n",
            " [0.43865633 0.72369203]\n",
            " [0.97284975 0.53230935]\n",
            " [0.47095693 0.03791046]]\n",
            "Final hidden bias: [[0.23761466]\n",
            " [0.50030317]\n",
            " [0.664244  ]\n",
            " [0.73262481]]\n",
            "Final output weights: [[ 0.08755892 -0.26240671 -0.06434509  0.57949139]]\n",
            "Final output bias: [[-0.23311018]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYc0VdjZuafl"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}